\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}
\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}
\usepackage{amssymb}
\usepackage{amsmath,amsthm}
\usepackage{float}
\usepackage{url}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{amsthm} 
\usepackage[ruled,vlined,linesnumbered]{algorithm2e} 
\usepackage{makecell}


\onehalfspacing
\usepackage{pdfpages}
\usepackage{times}
\usepackage{multirow}
\usepackage[toc,page]{appendix}
\usepackage{listings}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}[lemma]{Definition}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{example}[lemma]{Example}
\newtheorem{proposition}[lemma]{Proposition}


\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\support}{supp}
\DeclareMathOperator{\Trim}{Trim}
\DeclareMathOperator{\LTrim}{LTrim}

\begin{document}

\begin{definition}\label{consecutive} For a set $S \subseteq \mathbb{R}$ we say that
$B \subseteq S$ is \emph{consecutive} if any $s\in S$ that is smaller than $\max B$ and larger than $\min B$ is in $B$.
\end{definition}

\begin{definition}\label{part_consecutive} A partition $P=\{B_1,\dots,B_n\}$ of a set $S \subseteq \mathbb{R}$ is called 
\emph{consecutive} if all the subsets $B_1 ,\dots,B_n$ are consecutive.
\end{definition}


\begin{definition}\label{partition}
	For a discrete real random variable $X$ and a partition $P$ of its support, we define a new discrete random variable $X_P$ by:
	
	$$Pr(X_P = t) = \begin{cases} 
	Pr(X\in B) & \text{If $t$ is the minimum of some } B \in P, \\
	0      & \text{otherwise.}
	\end{cases}$$
\end{definition}


\begin{definition}
	For discrete real-valued variables $X_1$ and $X_2$, we say that $X_2$ is a one-sided Kolmogorov approximation, with the parameters $\varepsilon$ and $m$, of $X_1$, denoted by 
	$X_1 \preceq_{\varepsilon,n} X_2$, if, for all $t$, $0 \leq   F_{X_2}(t)-F_{X_1}(t) \leq \varepsilon$ and the size of the support of $X_2$ is smaller or equal to $m$. We say that the approximation is tight if there exists no  $X_2' \neq X_2$ such that $X_1 \preceq_{\varepsilon,m} X_2'$ and, for all $t$, $F_{X_2'}(t) \leq F_{X_2}(t)$. 
\end{definition}

\begin{proposition}
	For a discrete real-valued random variable $X$ and any $m \in \mathbb{N}$, 
	let $\varepsilon = \min\{\varepsilon \colon \text{ there is $X'$ such that $X \preceq_{\varepsilon,m} X'$}\}$ be the best possible approximation error for $X$ with a random variable whose support of size $m$. This same level of approximation can be obtained with a partition, i.e., there is consecutive partition $P$ of $\supp(X)$ such that $X \preceq_{\varepsilon,m} X_P$.
	
\end{proposition}

\begin{proof}
Let $X'$ be such that $X \preceq_{\varepsilon,m} X'$. Assume that $t_0,t_1,\dots,t_n$ are all the elements in the support of $X$ in ascending order. Define the random variable $X''$ by 
$$
P_{X''}(t) = \begin{cases}
P(X' \leq t_0)           & \text{if } t=t_0  \\ 
P(t_{i-1} < X' \leq t_i) & \text{if } t=t_i \text{ for some } i \neq 0 \\ 
0                        & \text{otherwise}
\end{cases}
$$
We will show now that: (1) $\support(X'') \subseteq \support(X')$ and (2) $X \preceq_{\varepsilon,m} X''$. \todo{Add proofs of these claims}

Let $s_0,s_1,\dots,s_m$ be the elements in the support of $X''$ in ascending order. Define the random variable $X'''$ by:
$$
P_{X'''}(t) = \begin{cases}
P(X \leq s_0)           & \text{if } t=s_0  \\ 
P(s_{i-1} < X \leq s_i) & \text{if } t=s_i \text{ for some } i \neq 0 \\ 
0                       & \text{otherwise}
\end{cases}
$$
We will show now that: (1) there is a partition $P$ such that $X'''=X_P$; (2) $X \preceq_{\varepsilon,m} X'''$. \todo{Add proofs of these claims}
\end{proof}


\begin{theorem}
Given $X_1 \dots X_k$ discrete random variables in a table representation and $m \in \mathbb{N}$, if $X'=OptimalTrim(X_1 \dots X_k,m)$, a discrete random variable and a partition of $X$ with a support of size $\leq m$ then $\max\limits_{t}|F_{\sum_1^k X_i}(t)-F_{X'}(t)|$ is minimal.
\end{theorem}

\begin{proof}
From Lemma~\ref{consecutive} we get that the optimal partition for the objective function $|F_{\sum_1^k X_i}(t)-F_{X'}(t)|$ is consecutive.
Using this fact allow us to reduce our problem to the same problem solved by Rothblum in the paper from 1982. Let $X = \sum_{i=1}^k X_i$, from Rothblum we first construct the weighted graph $G = (V,E)$, where $V = \support(X), E = \{\forall i,j \in V, (i,j)\in E\}$ and the weights $\forall e=(i,j)\in E, w(e) = |F_{X}(i)-F_{X}(j)|$. In a classical Rothblum the shortest path is found by Bellman Ford algorithm, in our case we use a variation of Bellman-Ford, minimax bottleneck path by using the function $bottleneck(x) =    min   [max(bottleneck(v),w(e))]$. Bellman-Ford algorithm can be forced to return path of some length, in our case $m$, meaning, find the lightest maximal edge in a length $m$ path. 
$X'$ is the result of $OptimalTrim(X_1 \dots X_n,m)$, which is the same as described above. 
\end{proof}

\begin{algorithm}
  \DontPrintSemicolon
  \SetKwFunction{Sequence}{Sequence} 
  \SetKwFunction{Convolv}{Conv}
  \SetKwFunction{Trim}{Trim}

   $D=((0,1))$ //  Dummy random var.: $0$ with prob. $1$ \;
  \For{$i=1$ \emph{\KwTo} $k$} {
	$D$=Convolve($D, X_i$)\; 
	}
   $G=(E,V)$ \;
   $V = \support(D)$\;
   $E = \{\forall i,j \in V, (i,j)\in E\}$\;
   \ForEach{$e=(i,j) \in  E $ }{
   $w(e) = F_D(i)+\sum_{t=i}^j P_D(t)-F_D(i)$
   }
   $L, \varepsilon$=BellmanFord($G$, $min(V)$, $m$, $bottleneck(x) =    min   [max(bottleneck(v),w(e))]$) /* Run Bellman-Ford algorithm as minimax bottleneck path version, $m$ times, starting with a single source, where $L$ is the path */ \;
   %http://www.cs.cmu.edu/afs/cs/academic/class/15451-f04/www/lectures/lect1012.txt
   $D'=()$\;
   \ForEach{$e=(i,j) \in  L $ }{
   $D' = append(D', (i,\sum_{t=i}^j P_D(t))$
   }
   \Return $D, bottleneck(x)$\;
  %-------------------------------%
  \SetKwProg{myproc}{Procedure}{}{}
  \myproc{
  BellmanFord($G$,$source$,$m$,$f$)
  }{ }
     
\caption{OptimalTrim ($X_1,\dots,X_k$ , $m$)}  
\label{alg:sequence}
\end{algorithm}


Two not proved yet issues:
1) why the first gives us the minimal partition? 2) bottleneck  = the maximal error, $\varepsilon$.
\end{document}

